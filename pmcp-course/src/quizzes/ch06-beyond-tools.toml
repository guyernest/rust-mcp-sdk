# Quiz: Resources, Prompts, and Workflows

id = "ch06-beyond-tools"
title = "Resources, Prompts, and Workflows"
lesson_id = "ch06"
pass_threshold = 0.7

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What is the key difference between MCP Resources and Tools?"
prompt.distractors = [
    "Resources are faster than tools",
    "Tools support JSON but resources don't",
    "Resources require authentication but tools don't"
]
answer.answer = "Resources provide stable, addressable data; Tools perform actions that may have side effects"
context = """
Resources are like bookmarks - stable data with a URI identity (db://schema).
Tools are like form submissions - actions with potential side effects. The AI
treats them differently: reading resources freely for context, but calling tools
deliberately to accomplish goals.
"""
id = "d6a1b2c3-e5f6-7890-abcd-ef1234560601"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "Why are Prompts described as the 'primary mechanism for user-controlled workflows'?"
prompt.distractors = [
    "Prompts are faster than tools",
    "Prompts bypass AI decision-making entirely",
    "Prompts are required by MCP protocol"
]
answer.answer = "Users explicitly invoke prompts to select a workflow, giving them predictable control over AI behavior"
context = """
Unlike tools (AI decides when to use) and resources (AI reads for context),
prompts are explicitly invoked by users. When a user types /quarterly-analysis,
they're choosing that specific workflow. This gives users control rather than
hoping the AI picks the right approach.
"""
id = "d6a1b2c3-e5f6-7890-abcd-ef1234560602"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What should you use for database schema information - a Resource or a Tool?"
prompt.distractors = [
    "A tool called 'get_schema'",
    "Either one works equally well",
    "A prompt that fetches the schema"
]
answer.answer = "A Resource like 'db://schema' because schemas are stable, addressable data"
context = """
Schema information is stable reference data with a clear identity - perfect for
a resource. The AI can read db://schema proactively for context without counting
it as an 'action'. A tool called 'get_schema' implies an action, but reading
schema is just gathering information.
"""
id = "d6a1b2c3-e5f6-7890-abcd-ef1234560603"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What is the guiding principle for choosing between soft and hard workflows?"
prompt.distractors = [
    "Use soft workflows by default because they're more flexible",
    "Use hard workflows only for simple single-step operations",
    "Let the AI decide which type of workflow to use"
]
answer.answer = "Do as much as possible on the server side, and allow the AI to complete the workflow if you can't"
context = """
Hard workflows execute on the server in a single round-trip with deterministic
results. Soft workflows require multiple round-trips with the AI interpreting
text guidance. The guiding principle is to push as much as possible to the server
side, falling back to soft or hybrid workflows only when LLM reasoning is genuinely required.
"""
id = "d6a1b2c3-e5f6-7890-abcd-ef1234560604"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What is the rule of thumb for deciding between Resources and Tools?"
prompt.distractors = [
    "Use resources for anything JSON-formatted",
    "Use tools for anything requiring parameters",
    "Use resources for read-only, tools for write"
]
answer.answer = "If you'd bookmark it, use a Resource; if you'd submit a form, use a Tool"
context = """
The bookmark/form metaphor captures the distinction well. Bookmarks point to
stable, addressable content (resources). Forms submit data and trigger actions
(tools). Database schema is a bookmark. Query execution is a form submission.
"""
id = "d6a1b2c3-e5f6-7890-abcd-ef1234560605"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "In a SequentialWorkflow, what is the difference between a step name and a binding name?"
prompt.distractors = [
    "They are the same thing - interchangeable terms",
    "Step names are for tools, binding names are for resources",
    "Binding names are automatically generated from step names"
]
answer.answer = "Step name identifies the step; binding name (from .bind()) is what other steps reference with from_step()"
context = """
This is a critical distinction. WorkflowStep::new('analyze', ...).bind('analysis_result')
creates step 'analyze' with binding 'analysis_result'. Other steps must use
from_step('analysis_result') - NOT from_step('analyze'). Using the step name
instead of the binding name is a common validation error.
"""
id = "d6a1b2c3-e5f6-7890-abcd-ef1234560606"

[[questions]]
type = "ShortAnswer"
prompt.prompt = "In Claude Desktop, how do users invoke MCP prompts?"
answer.answer = "slash commands"
answer.alternatives = ["/commands", "/ commands", "Slash commands", "/", "slash command"]
context = """
Claude Desktop exposes MCP prompts as slash commands. Users type /quarterly-analysis
or /customer-health-check to invoke specific workflows. Other clients may show
prompts differently - VS Code uses command palette, ChatGPT shows conversation starters.
"""
id = "d6a1b2c3-e5f6-7890-abcd-ef1234560607"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What are the main advantages of hard workflows over soft workflows?"
prompt.distractors = [
    "Hard workflows support more complex logic",
    "Hard workflows work with more MCP clients",
    "Hard workflows require less code to implement"
]
answer.answer = "Single round-trip, deterministic execution, automatic data binding, and pure function testing"
context = """
Hard workflows execute entirely server-side: one round-trip instead of 6+ for
a 3-step soft workflow, deterministic order (server enforces it), automatic
data binding between steps, and testable without AI. Soft workflows require
multiple LLM calls and may execute steps in unpredictable order.
"""
id = "d6a1b2c3-e5f6-7890-abcd-ef1234560608"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "In the SequentialWorkflow DSL, which helper function passes the entire output from a previous step?"
prompt.distractors = [
    "prompt_arg('step_name')",
    "field('binding', 'output')",
    "constant(json!({...}))"
]
answer.answer = "from_step('binding_name')"
context = """
The DSL provides four data source helpers: prompt_arg() for workflow arguments,
from_step() for entire previous output, field() for specific fields of previous
output, and constant() for fixed values. from_step('sales_data') passes everything
from a step that bound to 'sales_data'.
"""
id = "d6a1b2c3-e5f6-7890-abcd-ef1234560609"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "Why should you embed resources in hybrid workflow steps using .with_resource()?"
prompt.distractors = [
    "It's required by the MCP protocol for all workflows",
    "It reduces the number of tool calls needed",
    "It improves server-side caching performance"
]
answer.answer = "It provides the AI with exactly the documentation it needs to complete incomplete steps correctly"
context = """
Resource embedding is a powerful lever for MCP developers. When a step can't be
fully automated (fuzzy matching, user clarification), embedding relevant resources
like schemas, validation rules, and format templates gives the AI the exact
context it needs. Instead of guessing which resources to read, the AI receives
pre-selected documentation from the developer who knows the domain.
"""
id = "d6a1b2c3-e5f6-7890-abcd-ef1234560610"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "When does workflow validation occur in the PMCP SDK?"
prompt.distractors = [
    "Only when you explicitly call .validate()",
    "At runtime when a user invokes the workflow",
    "During cargo build compilation"
]
answer.answer = "Automatically when you register with .prompt_workflow() - if invalid, the server won't build"
context = """
Workflow validation is automatic and fail-fast. When you call .prompt_workflow(workflow),
the builder validates the workflow structure - checking for unknown bindings,
undefined arguments, and invalid mappings. If validation fails, registration
returns an error and the server won't start. You see errors immediately at
startup, not when users invoke the workflow.
"""
id = "d6a1b2c3-e5f6-7890-abcd-ef1234560611"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "When should you use a hybrid workflow instead of a fully hard workflow?"
prompt.distractors = [
    "When you want to reduce server load",
    "When the workflow has more than 3 steps",
    "When you need to support multiple MCP clients"
]
answer.answer = "When some steps require LLM judgment like fuzzy matching or context-dependent decisions"
context = """
Hybrid workflows execute deterministic steps on the server, then hand off to the
AI for steps requiring reasoning. For example, 'add task to project' might list
pages server-side, but need the AI to fuzzy-match 'MCP Tester' to 'mcp-tester'.
Add .with_guidance() and .with_resource() to help the AI complete those steps.
"""
id = "d6a1b2c3-e5f6-7890-abcd-ef1234560612"
