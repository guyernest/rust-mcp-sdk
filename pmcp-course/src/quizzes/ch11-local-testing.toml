# Quiz: Local Testing

id = "ch11-local-testing"
title = "Local MCP Server Testing"
lesson_id = "ch11"
pass_threshold = 0.7

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What is the primary purpose of MCP Inspector?"
prompt.distractors = [
    "Running automated test suites in CI/CD pipelines for regression testing",
    "Generating test scenarios from schemas for comprehensive coverage",
    "Performance benchmarking MCP servers under production-like load"
]
answer.answer = "Interactive debugging and manual exploration of MCP server behavior during development"
answer.position = 0
context = """
MCP Inspector is a visual debugging tool that connects to MCP servers and provides
real-time protocol visibility, interactive tool execution, and schema exploration.
It excels at manual exploration and debugging during development, while mcp-tester
handles automated testing for CI/CD pipelines.
"""
id = "e11a2b3c-d4e5-6789-abcd-123456781101"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What testing approach does mcp-tester use to generate test scenarios?"
prompt.distractors = [
    "Manual test case definition only with no automation support",
    "Random fuzzing of all parameters without schema guidance",
    "Record and playback of user sessions from production traffic"
]
answer.answer = "Schema-driven test generation from the MCP server's JSON Schema"
answer.position = 1
context = """
mcp-tester generates test scenarios by introspecting the MCP server's JSON Schema.
It analyzes input schemas to automatically create valid inputs, invalid inputs,
edge cases, and type validation tests. This approach ensures comprehensive coverage
without manual test case writing.
"""
id = "e11a2b3c-d4e5-6789-abcd-123456781102"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "In the testing pyramid for MCP servers, which tests should be most numerous?"
prompt.distractors = [
    "End-to-end tests with MCP Inspector for full protocol validation",
    "mcp-tester integration tests for multi-tool workflow testing",
    "Claude Desktop user acceptance tests for real-world scenarios"
]
answer.answer = "Rust unit tests for tool logic and input validation"
answer.position = 2
context = """
The testing pyramid for MCP servers has unit tests at the base, which should be
the most numerous. These test tool logic in isolation and run fastest. mcp-tester
integration tests are in the middle, and E2E tests with Inspector or Claude Desktop
are at the top (fewest but most comprehensive).
"""
id = "e11a2b3c-d4e5-6789-abcd-123456781104"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What Rust crate enables property-based testing with random inputs?"
prompt.distractors = [
    "quicktest for rapid iterations and fast feedback cycles",
    "fuzzcheck for coverage-guided fuzzing with corpus management",
    "criterion for benchmarking and performance regression testing"
]
answer.answer = "proptest for generating random inputs that satisfy properties"
answer.position = 3
context = """
The proptest crate enables property-based testing in Rust, generating random inputs
that should satisfy invariants. It helps catch edge cases that manual tests miss.
Use proptest! macro to define property tests with input ranges, then assert that
properties hold for all generated values.
"""
id = "e11a2b3c-d4e5-6789-abcd-123456781105"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What file format does mcp-tester use for test scenario definitions?"
prompt.distractors = [
    "JSON files with test cases and expected outcomes",
    "TOML files with assertions and test metadata",
    "Rust source files with test functions and macros"
]
answer.answer = "YAML files with steps, inputs, and assertions"
answer.position = 0
context = """
mcp-tester uses YAML format for test scenario files. Each file contains a name,
description, tags, and a list of steps. Each step specifies a tool name, input
parameters, and expected outcomes (assertions). YAML's readability makes it easy
to review and edit generated scenarios.
"""
id = "e11a2b3c-d4e5-6789-abcd-123456781106"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What is the purpose of variable capture in mcp-tester scenarios?"
prompt.distractors = [
    "Storing test results for later analysis and reporting",
    "Caching responses to speed up tests and reduce server load",
    "Recording session replays for debugging failed test runs"
]
answer.answer = "Extracting values from responses to use in subsequent test steps"
answer.position = 1
context = """
Variable capture allows extracting values from one step's response to use in
later steps. Use capture: { var_name: '$.path.to.value' } with JSONPath syntax,
then reference as ${var_name} in subsequent inputs. This enables multi-step
workflows like Create → Read → Update → Delete testing.
"""
id = "e11a2b3c-d4e5-6789-abcd-123456781108"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What test category is generated for boundary values like minimum and maximum integers?"
prompt.distractors = [
    "_valid tests for happy path scenarios with expected inputs",
    "_invalid tests for error conditions and rejection cases",
    "_types tests for type coercion and format validation"
]
answer.answer = "_edge tests for boundary conditions and extreme values"
answer.position = 2
context = """
Schema-driven generation creates _edge tests for boundary conditions including
minimum/maximum values (integers at INT_MIN/INT_MAX), empty strings, maximum-length
strings, arrays at minimum/maximum size, and values just inside/outside boundaries.
These catch off-by-one errors and overflow conditions.
"""
id = "e11a2b3c-d4e5-6789-abcd-123456781110"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What attribute marks a Rust function as an async test with tokio runtime?"
prompt.distractors = [
    "#[test] with async fn declaration for basic async support",
    "#[async_test] from async-std for alternative runtime",
    "#[runtime::test] for multi-threaded concurrent execution"
]
answer.answer = "#[tokio::test] which sets up a tokio runtime for the test"
answer.position = 3
context = """
Use #[tokio::test] to write async tests with the tokio runtime. This attribute
creates a runtime for each test, handles .await calls, and supports async setup
and teardown. For database tests or HTTP clients, this is essential since most
MCP operations are async.
"""
id = "e11a2b3c-d4e5-6789-abcd-123456781111"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "When should you use MCP Inspector vs mcp-tester?"
prompt.distractors = [
    "Inspector for CI/CD automation, mcp-tester for local development",
    "Inspector for performance testing, mcp-tester for interactive debugging",
    "Inspector for regression testing, mcp-tester for codebase exploration"
]
answer.answer = "Inspector for interactive debugging during development, mcp-tester for automated CI/CD testing"
answer.position = 0
context = """
Use MCP Inspector during development for interactive debugging, exploring schemas,
testing individual tools, and reproducing bugs. Use mcp-tester for automated
testing in CI/CD pipelines, regression testing, and comprehensive test coverage.
They complement each other in the development workflow.
"""
id = "e11a2b3c-d4e5-6789-abcd-123456781113"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What output format should mcp-tester use for CI/CD integration?"
prompt.distractors = [
    "JSON for machine readability and API consumption",
    "TAP (Test Anything Protocol) for Unix compatibility",
    "Plain text for human readability in terminal output"
]
answer.answer = "JUnit XML format which is supported by all major CI systems"
answer.position = 1
context = """
JUnit XML is the standard format for CI/CD test results. Use --format junit
--output results.xml to generate JUnit format. GitHub Actions, GitLab CI,
Jenkins, and other CI systems can parse JUnit XML to display test results,
track trends, and annotate PRs with failures.
"""
id = "e11a2b3c-d4e5-6789-abcd-123456781114"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What does the proptest macro prop_assert! do differently from assert!?"
prompt.distractors = [
    "It runs assertions in parallel for speed and concurrency",
    "It logs assertion failures to a file for later analysis",
    "It retries failed assertions automatically with backoff"
]
answer.answer = "It reports failures as shrunk minimal test cases instead of panicking"
answer.position = 2
context = """
prop_assert! works with proptest's shrinking mechanism. When an assertion fails,
proptest automatically tries to find the smallest/simplest input that still
causes the failure. This makes debugging easier since you see minimal failing
examples rather than complex random values.
"""
id = "e11a2b3c-d4e5-6789-abcd-123456781115"
