# Quiz: Deployment Overview

id = "ch07-deployment"
title = "Remote MCP Deployment"
lesson_id = "ch07"
pass_threshold = 0.7

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What is the primary advantage of remote MCP deployments over local servers for business users?"
prompt.distractors = [
    "Remote servers are faster than local servers due to better hardware",
    "Remote servers support more MCP features than local deployments",
    "Remote servers use less memory by sharing resources across users"
]
answer.answer = "Business users can access MCP tools without installing Rust, configuring databases, or managing server processes"
answer.position = 1
context = """
Local MCP servers require technical setup on each user's machine - Rust toolchain,
database credentials, process management. Remote deployment transforms your MCP
server into a managed service that sales, analysts, and support teams can access
through Claude.ai without any technical setup.
"""
id = "a7b1c2d3-e4f5-6789-abcd-ef1234560701"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "Why should MCP servers run 'close to the data' they access?"
prompt.distractors = [
    "To reduce cloud provider costs through data transfer optimization",
    "To avoid firewall configuration and complex networking rules",
    "To simplify deployment scripts by using the same environment"
]
answer.answer = "Network latency to databases is ~1ms in the same VPC versus ~50-200ms from a user's laptop, making queries 100x faster"
answer.position = 2
context = """
MCP servers often make multiple database queries per tool call. When running in
the same AWS VPC as RDS, latency is ~1ms per query. From a user's laptop over
the internet, it's 50-200ms. For 10 queries, that's 10ms vs 500-2000ms - a
dramatic difference in user experience.
"""
id = "a7b1c2d3-e4f5-6789-abcd-ef1234560702"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What is the key difference between Lambda's and Cloud Run's concurrency models?"
prompt.distractors = [
    "Lambda supports async/await but Cloud Run doesn't support async patterns",
    "Cloud Run has faster cold starts than Lambda due to lighter isolation",
    "Lambda uses containers but Cloud Run uses functions for lighter deployment"
]
answer.answer = "Lambda handles 1 request per instance (scales horizontally); Cloud Run handles many concurrent requests per container"
answer.position = 3
context = """
Lambda spawns a new instance for each concurrent request - 100 concurrent requests
means 100 Lambda instances. Cloud Run runs a container that handles up to 80
concurrent requests internally using async runtime. This affects scaling behavior
and how you design stateful operations.
"""
id = "a7b1c2d3-e4f5-6789-abcd-ef1234560703"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "Why does Rust have significantly lower cold start times than Python on AWS Lambda?"
prompt.distractors = [
    "Rust uses a faster JIT compiler that optimizes code at runtime",
    "Lambda gives Rust more memory by default for better performance",
    "Python requires more API Gateway configuration for routing"
]
answer.answer = "Rust compiles to native machine code with no interpreter startup or JIT warmup required"
answer.position = 0
context = """
Rust cold starts are ~50-100ms on Lambda versus ~500-1500ms for Python. Rust
compiles to a native binary that starts immediately. Python must load the
interpreter, import modules, and warm up. This directly translates to lower
costs (Lambda charges per GB-second) and better user experience.
"""
id = "a7b1c2d3-e4f5-6789-abcd-ef1234560704"

[[questions]]
type = "ShortAnswer"
prompt.prompt = "What is Cloudflare Workers' compilation target that allows Rust code to run at the edge?"
answer.answer = "WebAssembly"
answer.alternatives = ["WASM", "wasm", "Web Assembly", "webassembly"]
context = """
Cloudflare Workers runs WebAssembly (WASM) on V8 isolates at 300+ edge locations.
Rust has first-class WASM support via the wasm32-unknown-unknown target. The
compiled WASM runs at near-native speed with sub-millisecond cold starts because
V8 isolates spin up in microseconds, not the hundreds of milliseconds containers require.
"""
id = "a7b1c2d3-e4f5-6789-abcd-ef1234560705"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What is the correct way to store database credentials for a Lambda MCP server?"
prompt.distractors = [
    "In .pmcp/deploy.toml under [lambda.environment] for easy configuration",
    "In a .env file committed to the repository for version control tracking",
    "Hardcoded in the Rust source code with #[cfg(prod)] for compile-time injection"
]
answer.answer = "In AWS Secrets Manager, retrieved at runtime with credentials cached in a static OnceCell"
answer.position = 1
context = """
Never store secrets in code, config files, or environment variables that appear in
logs. Use AWS Secrets Manager to store credentials securely. In Lambda, retrieve
secrets during cold start and cache them in a static OnceCell for reuse across
warm invocations. This keeps secrets out of source control and config files.
"""
id = "a7b1c2d3-e4f5-6789-abcd-ef1234560709"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What is the purpose of placing a Lambda function in a VPC private subnet?"
prompt.distractors = [
    "To improve Lambda cold start performance through pre-warming",
    "To enable OAuth authentication with Cognito user pools",
    "To reduce data transfer costs between services"
]
answer.answer = "To access private resources like RDS databases that have no public internet exposure"
answer.position = 2
context = """
VPC-attached Lambda functions can reach resources in private subnets - like RDS
databases configured with publiclyAccessible: false. The database never exposes
a public endpoint; only resources within the VPC (or peered VPCs) can connect.
This is a critical security boundary for production deployments.
"""
id = "a7b1c2d3-e4f5-6789-abcd-ef1234560710"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What is the difference between authentication and authorization in MCP security?"
prompt.distractors = [
    "Authentication uses OAuth while authorization uses API keys for access",
    "Authentication is server-side while authorization is checked client-side",
    "Authentication is for tools while authorization applies only to resources"
]
answer.answer = "Authentication verifies WHO is making the request; authorization determines WHAT they can access"
answer.position = 3
context = """
Authentication (OAuth tokens, API keys) establishes identity - this request is from
user alice@company.com. Authorization checks permissions - does Alice have the
'mcp:write' scope to call the delete_item tool? Both are required: knowing who
made the request is useless without enforcing what they're allowed to do.
"""
id = "a7b1c2d3-e4f5-6789-abcd-ef1234560711"

[[questions]]
type = "ShortAnswer"
prompt.prompt = "What SQL vulnerability is prevented by using parameterized queries instead of string concatenation?"
answer.answer = "SQL injection"
answer.alternatives = ["sql injection", "SQLi", "sqli", "injection", "SQL Injection"]
context = """
String concatenation like format!('SELECT * FROM users WHERE name = '{}'', input.name)
allows attackers to inject SQL: input '; DROP TABLE users; --. Parameterized
queries separate SQL structure from data - the database treats user input as
literal values, never as SQL commands. Always use .bind() with sqlx.
"""
id = "a7b1c2d3-e4f5-6789-abcd-ef1234560712"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "Which deployment target has the fastest cold starts at ~0-5ms?"
prompt.distractors = [
    "AWS Lambda with provisioned concurrency for pre-warmed instances",
    "Google Cloud Run with minimum instances kept always running",
    "AWS Lambda with ARM64 architecture for faster binary execution"
]
answer.answer = "Cloudflare Workers because V8 isolates spin up in microseconds, not milliseconds"
answer.position = 0
context = """
Workers uses V8 isolates, not containers or microVMs. Isolates are extremely
lightweight - they share the V8 engine and spin up in microseconds. Lambda
cold starts are 50-150ms (microVM + runtime init), Cloud Run is 100-500ms
(container startup). Workers' sub-millisecond cold starts are unmatched.
"""
id = "a7b1c2d3-e4f5-6789-abcd-ef1234560714"
