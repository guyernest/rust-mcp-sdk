# Quiz: Effective AI Collaboration

id = "ch16-collaboration"
title = "Effective AI Collaboration for MCP Development"
lesson_id = "ch16"
pass_threshold = 0.7

[[questions]]
type = "MultipleChoice"
prompt.prompt = "In the cargo-pmcp workflow, what is the correct order of steps?"
prompt.distractors = [
    "Implement → Scaffold → Test → Deploy without quality validation",
    "Test → Implement → Scaffold → Validate using manual review process",
    "Deploy → Test → Implement → Scaffold with hotfix-first approach"
]
answer.answer = "Scaffold → Implement → Dev Server → Test → Quality Gates → Production"
answer.position = 0
context = """
The correct workflow is: 1) Scaffold with cargo pmcp new/add server, 2) Implement
tools in crates/mcp-*-core/src/tools/, 3) Start dev server with cargo pmcp dev,
4) Generate and run tests, 5) Pass quality gates (fmt, clippy, test), 6) Build
for production. Never skip scaffolding or quality gates.
"""
id = "f16a1b2c-d3e4-5678-abcd-123456780201"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What makes a good prompt for requesting an MCP tool?"
prompt.distractors = [
    "Specifying exact struct names and function signatures for consistency",
    "Writing the code yourself and asking AI to review for improvements",
    "Using technical Rust jargon throughout for precise communication"
]
answer.answer = "Describing what the tool does, its inputs/outputs, and error cases without dictating implementation"
answer.position = 1
context = """
Good prompts specify WHAT (purpose), inputs (required/optional fields), outputs
(structure), and error cases. Let AI decide HOW to implement. Bad prompts either
micromanage implementation details or are too vague ('make a database thing').
Focus on behavior and constraints, not code structure.
"""
id = "f16a1b2c-d3e4-5678-abcd-123456780202"

[[questions]]
type = "ShortAnswer"
prompt.prompt = "What is the quality gate command to check for code style issues in Rust?"
answer.answer = "cargo clippy"
answer.alternatives = ["cargo clippy -- -D warnings", "clippy"]
context = """
cargo clippy is Rust's lint tool that catches code smells, inefficiencies, and
anti-patterns. Run with '-- -D warnings' to treat warnings as errors. Common fixes
include removing redundant clones, simplifying match expressions, and using more
idiomatic patterns. Zero warnings should be the goal.
"""
id = "f16a1b2c-d3e4-5678-abcd-123456780203"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What is the role of the developer vs AI in the collaboration model?"
prompt.distractors = [
    "Developer writes code, AI reviews it for style and correctness",
    "AI decides architecture, developer implements the detailed logic",
    "Developer tests, AI deploys to staging and production environments"
]
answer.answer = "Developer defines WHAT to build and domain knowledge; AI implements HOW and iterates on compiler feedback"
answer.position = 2
context = """
Developers provide requirements, domain expertise, architectural decisions, and
code review. AI generates implementation code, applies cargo-pmcp patterns, handles
boilerplate, and iterates until quality gates pass. This division lets developers
focus on business value while AI handles implementation details.
"""
id = "f16a1b2c-d3e4-5678-abcd-123456780204"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What are the five layers of quality assurance in AI-assisted MCP development?"
prompt.distractors = [
    "Unit, Integration, E2E, Performance, Security testing pyramid",
    "Lint, Build, Test, Deploy, Monitor continuous delivery pipeline",
    "Code, Review, Merge, Release, Rollback standard git workflow"
]
answer.answer = "Compile-time (cargo build), Static analysis (clippy), Unit tests, Integration tests (cargo pmcp test), Code review"
answer.position = 3
context = """
The five QA layers are: 1) cargo build catches type errors, 2) cargo clippy catches
code smells, 3) cargo test catches logic errors, 4) cargo pmcp test catches MCP
protocol issues, 5) Human review catches design flaws. Each layer catches different
problems; all are necessary for production quality.
"""
id = "f16a1b2c-d3e4-5678-abcd-123456780205"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "When should you share compiler error messages with the AI?"
prompt.distractors = [
    "Only for critical errors, not warnings that can be ignored",
    "Never - AI should discover errors itself through compilation",
    "Only if the error is unfamiliar and you need explanation"
]
answer.answer = "Always - specific error messages enable targeted fixes instead of guessing"
answer.position = 0
context = """
Sharing exact compiler errors helps AI apply targeted fixes. 'That doesn't work'
forces AI to guess, while 'error[E0308]: mismatched types at line 25' enables
precise correction. Copy the full error message including code context and
suggestions for best results.
"""
id = "f16a1b2c-d3e4-5678-abcd-123456780206"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What is a prompt anti-pattern that should be avoided?"
prompt.distractors = [
    "Specifying input/output types to clarify the tool's interface",
    "Describing error handling requirements for robust implementations",
    "Listing multiple tools for a server to define scope"
]
answer.answer = "Micromanaging implementation by specifying exact struct names, field types, and function signatures"
answer.position = 1
context = """
Micromanaging (e.g., 'Create struct WeatherInput with field city of type String...')
constrains AI unnecessarily and may lead to suboptimal solutions. Describe the
behavior: 'A tool that takes a city name and returns weather data'. Let AI choose
idiomatic implementations using its knowledge of pmcp patterns.
"""
id = "f16a1b2c-d3e4-5678-abcd-123456780208"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What should be included in error messages for MCP tools?"
prompt.distractors = [
    "Stack traces and internal state for debugging by developers",
    "The user's IP address for logging and security tracking",
    "Only a generic error code to avoid exposing implementation details"
]
answer.answer = "What went wrong, why it matters, and how to fix it (if possible)"
answer.position = 2
context = """
Good error messages explain: 1) What happened ('File size 15MB exceeds limit'),
2) The constraint ('Maximum is 10MB'), 3) How to fix it ('Reduce file size or split').
This helps AI clients auto-recover or provide helpful user feedback. Avoid vague
messages like 'invalid input' or 'error'.
"""
id = "f16a1b2c-d3e4-5678-abcd-123456780209"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What is the purpose of integration testing with cargo pmcp test?"
prompt.distractors = [
    "Test individual Rust functions in isolation using mock dependencies",
    "Measure code coverage percentages to ensure testing completeness",
    "Deploy to staging environment for pre-production validation testing"
]
answer.answer = "Test full MCP server behavior including protocol interactions and tool execution"
answer.position = 3
context = """
cargo pmcp test runs YAML scenarios against the running dev server, testing the
complete MCP protocol flow: connection, tool listing, tool execution, and responses.
This catches issues that unit tests miss: serialization problems, transport errors,
and protocol violations. Run with --generate-scenarios first.
"""
id = "f16a1b2c-d3e4-5678-abcd-123456780210"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What template should be used when building a custom MCP server with unique tools?"
prompt.distractors = [
    "calculator - has the most features and comprehensive documentation",
    "sqlite_explorer - most production-ready with real database integration",
    "complete_calculator - best for learning MCP patterns and conventions"
]
answer.answer = "minimal - provides empty structure without example tools to customize"
answer.position = 0
context = """
The minimal template creates the directory structure, module organization, and
transport setup without any example tools. Use it when building custom servers
where you'll implement all tools yourself. calculator and sqlite_explorer templates
include example implementations useful for learning but not for custom projects.
"""
id = "f16a1b2c-d3e4-5678-abcd-123456780212"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "How should you handle optional input parameters in TypedTool?"
prompt.distractors = [
    "Use default values in the struct definition with Default trait",
    "Use String::new() for missing values to avoid null handling",
    "Make all parameters required and validate at the client level"
]
answer.answer = "Use Option<T> for the field type and handle with unwrap_or or if let Some"
answer.position = 1
context = """
Optional parameters should be Option<T> in the input struct. In the handler, use
unwrap_or(default) for simple defaults or if let Some(value) for conditional logic.
JsonSchema generates the correct schema marking the field as optional. Don't use
empty strings or magic values for missing data.
"""
id = "f16a1b2c-d3e4-5678-abcd-123456780213"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What is the recommended approach for testing edge cases in MCP tools?"
prompt.distractors = [
    "Only test the happy path, edge cases are rare in production",
    "Test edge cases manually in production using real user data",
    "Let users report edge case bugs for prioritization in backlog"
]
answer.answer = "Add explicit tests for boundary values, empty inputs, Unicode, and documented error cases"
answer.position = 2
context = """
Request specific edge case tests: empty inputs, max length inputs, Unicode characters,
boundary values (0, -1, MAX), null/missing optionals, and each documented error case.
AI can generate these tests automatically when asked. Edge cases often reveal bugs
that happy-path testing misses.
"""
id = "f16a1b2c-d3e4-5678-abcd-123456780214"

[[questions]]
type = "MultipleChoice"
prompt.prompt = "What should you do after AI generates code for a new tool?"
prompt.distractors = [
    "Deploy immediately to test in production with feature flags",
    "Commit and push without testing to save development time",
    "Ask AI to explain but don't run tests until code review"
]
answer.answer = "Run quality gates (fmt, clippy, test) and review the code to understand what was generated"
answer.position = 3
context = """
Always run cargo fmt --check, cargo clippy -- -D warnings, and cargo test after
any code generation. Then review the code to understand what was built - you're
responsible for what goes into production. Ask AI to explain complex logic if
needed. Never deploy code you don't understand.
"""
id = "f16a1b2c-d3e4-5678-abcd-123456780216"
