# Exercise: Building a Database Query Tool
#
# This exercise introduces database integration - a critical skill for
# enterprise MCP servers that provide data access to AI assistants.

[exercise]
id = "ch03-01-db-query-basics"
title = "Building a Database Query Tool"
type = "implementation"
chapter = "ch03"
difficulty = "intermediate"
estimated_minutes = 35
prerequisites = ["ch02-01-hello-mcp", "ch02-02-calculator"]

[exercise.context]
description = """
Database access is the "killer app" for enterprise MCP servers. When employees
need data for AI conversations, they shouldn't have to export CSVs and paste
into chat windows. An MCP server can provide secure, direct access.

In this exercise, you'll build a database query tool that:
1. Lists available tables
2. Executes read-only SQL queries
3. Returns structured results

You'll be working with a real SQLite database - the same one powering the
course exercises you're working through right now!
"""

[exercise.learning_objectives]
thinking = [
    "Why read-only access is essential for AI tools",
    "How to structure database results for AI consumption",
    "The tradeoffs between flexibility and security in query tools",
]

doing = [
    "Create tools that interact with SQLite databases",
    "Use sqlx for async database operations",
    "Structure output for AI-friendly consumption",
]

[exercise.discussion]
prompts = [
    "Why might you want an AI to query databases directly instead of using pre-built reports?",
    "What's the risk of allowing arbitrary SQL queries? How would you mitigate it?",
    "How should results be formatted so an AI can understand and explain them?",
    "What metadata would help an AI write better queries?",
]

concepts = [
    "Connection pooling",
    "Async database operations",
    "Result serialization",
    "Schema introspection",
]

[exercise.starter_code]
language = "rust"
filename = "src/main.rs"
template = """
//! Database Query MCP Server
//!
//! Provides read-only access to a SQLite database through MCP tools.

use pmcp::{Server, ServerCapabilities, ToolCapabilities};
use pmcp::server::TypedTool;
use serde::{Deserialize, Serialize};
use schemars::JsonSchema;
use anyhow::Result;
use sqlx::{Pool, Sqlite, sqlite::SqlitePoolOptions, Row};
use std::sync::Arc;

type DbPool = Arc<Pool<Sqlite>>;

// TODO: Define input for list_tables tool (no parameters needed)
#[derive(Deserialize, JsonSchema)]
struct ListTablesInput {}

// Output structure for table information
#[derive(Serialize)]
struct TableInfo {
    name: String,
    row_count: i64,
}

// TODO: Define QueryInput
// Hint: Include the SQL query string and an optional row limit
#[derive(Deserialize, JsonSchema)]
struct QueryInput {
    /// The SQL query to execute (must be SELECT)
    query: String,
    /// Maximum rows to return (default: 100)
    #[serde(default = "default_limit")]
    limit: i32,
}

fn default_limit() -> i32 { 100 }

// Output structure for query results
#[derive(Serialize)]
struct QueryResult {
    columns: Vec<String>,
    rows: Vec<Vec<serde_json::Value>>,
    row_count: usize,
    truncated: bool,
}

/// List all tables in the database
async fn list_tables(pool: &DbPool) -> Result<Vec<TableInfo>> {
    // TODO: Query sqlite_master for table names
    // Then get row count for each table
    //
    // Hint: SQLite stores metadata in sqlite_master table
    // SELECT name FROM sqlite_master WHERE type='table'

    todo!("Implement list_tables")
}

/// Execute a read-only SQL query
async fn execute_query(pool: &DbPool, input: QueryInput) -> Result<QueryResult> {
    // TODO: Implement query execution
    //
    // Steps:
    // 1. Validate the query starts with SELECT
    // 2. Add LIMIT clause if not present
    // 3. Execute the query
    // 4. Extract column names and row data
    // 5. Check if results were truncated

    todo!("Implement execute_query")
}

#[tokio::main]
async fn main() -> Result<()> {
    // Connect to database
    let database_url = std::env::var("DATABASE_URL")
        .unwrap_or_else(|_| "sqlite:course.db".to_string());

    let pool: DbPool = Arc::new(
        SqlitePoolOptions::new()
            .max_connections(5)
            .connect(&database_url)
            .await?
    );

    let pool_for_tables = pool.clone();
    let pool_for_query = pool.clone();

    let server = Server::builder()
        .name("db-explorer")
        .version("1.0.0")
        .capabilities(ServerCapabilities {
            tools: Some(ToolCapabilities::default()),
            ..Default::default()
        })
        .tool("list_tables", TypedTool::new("list_tables", move |_: ListTablesInput| {
            let pool = pool_for_tables.clone();
            Box::pin(async move {
                let tables = list_tables(&pool).await?;
                Ok(serde_json::to_value(tables)?)
            })
        }))
        .tool("query", TypedTool::new("query", move |input: QueryInput| {
            let pool = pool_for_query.clone();
            Box::pin(async move {
                let result = execute_query(&pool, input).await?;
                Ok(serde_json::to_value(result)?)
            })
        }))
        .build()?;

    println!("Database explorer ready! Connected to: {}", database_url);
    Ok(())
}
"""

[exercise.tests]
language = "rust"
run_command = "cargo test"
test_code = """
#[cfg(test)]
mod tests {
    use super::*;

    async fn create_test_pool() -> DbPool {
        let pool = SqlitePoolOptions::new()
            .max_connections(1)
            .connect("sqlite::memory:")
            .await
            .unwrap();

        // Create test table
        sqlx::query("CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT, email TEXT)")
            .execute(&pool)
            .await
            .unwrap();

        sqlx::query("INSERT INTO users (name, email) VALUES ('Alice', 'alice@example.com')")
            .execute(&pool)
            .await
            .unwrap();

        sqlx::query("INSERT INTO users (name, email) VALUES ('Bob', 'bob@example.com')")
            .execute(&pool)
            .await
            .unwrap();

        Arc::new(pool)
    }

    #[tokio::test]
    async fn test_list_tables() {
        let pool = create_test_pool().await;
        let tables = list_tables(&pool).await.unwrap();

        assert!(!tables.is_empty());
        assert!(tables.iter().any(|t| t.name == "users"));
    }

    #[tokio::test]
    async fn test_query_basic() {
        let pool = create_test_pool().await;
        let input = QueryInput {
            query: "SELECT * FROM users".to_string(),
            limit: 100,
        };

        let result = execute_query(&pool, input).await.unwrap();

        assert_eq!(result.row_count, 2);
        assert_eq!(result.columns.len(), 3); // id, name, email
        assert!(!result.truncated);
    }

    #[tokio::test]
    async fn test_query_with_limit() {
        let pool = create_test_pool().await;
        let input = QueryInput {
            query: "SELECT * FROM users".to_string(),
            limit: 1,
        };

        let result = execute_query(&pool, input).await.unwrap();

        assert_eq!(result.row_count, 1);
        assert!(result.truncated);
    }

    #[tokio::test]
    async fn test_rejects_non_select() {
        let pool = create_test_pool().await;
        let input = QueryInput {
            query: "DELETE FROM users".to_string(),
            limit: 100,
        };

        let result = execute_query(&pool, input).await;
        assert!(result.is_err());
    }
}
"""

[exercise.hints]
level_1 = """
For list_tables, query sqlite_master:
```rust
let tables: Vec<(String,)> = sqlx::query_as(
    "SELECT name FROM sqlite_master WHERE type='table'"
)
.fetch_all(pool.as_ref())
.await?;
```
"""

level_2 = """
For execute_query, validate SELECT first:
```rust
let trimmed = input.query.trim().to_uppercase();
if !trimmed.starts_with("SELECT") {
    return Err(anyhow::anyhow!("Only SELECT queries are allowed"));
}
```
"""

level_3 = """
Complete list_tables:
```rust
async fn list_tables(pool: &DbPool) -> Result<Vec<TableInfo>> {
    let tables: Vec<(String,)> = sqlx::query_as(
        "SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'"
    )
    .fetch_all(pool.as_ref())
    .await?;

    let mut result = Vec::new();
    for (name,) in tables {
        let count: (i64,) = sqlx::query_as(&format!("SELECT COUNT(*) FROM {}", name))
            .fetch_one(pool.as_ref())
            .await?;
        result.push(TableInfo { name, row_count: count.0 });
    }
    Ok(result)
}
```
"""

hint_policy = "after_stuck"
stuck_threshold_minutes = 7

[exercise.solution]
unlock_policy = "after_attempt"
solution_code = """
use pmcp::{Server, ServerCapabilities, ToolCapabilities};
use pmcp::server::TypedTool;
use serde::{Deserialize, Serialize};
use schemars::JsonSchema;
use anyhow::Result;
use sqlx::{Pool, Sqlite, sqlite::SqlitePoolOptions, Row, Column};
use std::sync::Arc;

type DbPool = Arc<Pool<Sqlite>>;

#[derive(Deserialize, JsonSchema)]
struct ListTablesInput {}

#[derive(Serialize)]
struct TableInfo {
    name: String,
    row_count: i64,
}

#[derive(Deserialize, JsonSchema)]
struct QueryInput {
    query: String,
    #[serde(default = "default_limit")]
    limit: i32,
}

fn default_limit() -> i32 { 100 }

#[derive(Serialize)]
struct QueryResult {
    columns: Vec<String>,
    rows: Vec<Vec<serde_json::Value>>,
    row_count: usize,
    truncated: bool,
}

async fn list_tables(pool: &DbPool) -> Result<Vec<TableInfo>> {
    let tables: Vec<(String,)> = sqlx::query_as(
        "SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'"
    )
    .fetch_all(pool.as_ref())
    .await?;

    let mut result = Vec::new();
    for (name,) in tables {
        let count: (i64,) = sqlx::query_as(&format!("SELECT COUNT(*) FROM {}", name))
            .fetch_one(pool.as_ref())
            .await?;
        result.push(TableInfo { name, row_count: count.0 });
    }
    Ok(result)
}

async fn execute_query(pool: &DbPool, input: QueryInput) -> Result<QueryResult> {
    // Validate SELECT only
    let trimmed = input.query.trim().to_uppercase();
    if !trimmed.starts_with("SELECT") {
        return Err(anyhow::anyhow!("Only SELECT queries are allowed"));
    }

    // Add LIMIT if not present
    let query = if !trimmed.contains("LIMIT") {
        format!("{} LIMIT {}", input.query, input.limit + 1)
    } else {
        input.query.clone()
    };

    // Execute query
    let rows = sqlx::query(&query)
        .fetch_all(pool.as_ref())
        .await?;

    // Check truncation
    let truncated = rows.len() > input.limit as usize;
    let rows: Vec<_> = rows.into_iter().take(input.limit as usize).collect();

    // Extract columns
    let columns: Vec<String> = if !rows.is_empty() {
        rows[0].columns().iter().map(|c| c.name().to_string()).collect()
    } else {
        vec![]
    };

    // Convert to JSON
    let json_rows: Vec<Vec<serde_json::Value>> = rows
        .iter()
        .map(|row| {
            (0..columns.len()).map(|i| {
                // Try different types
                if let Ok(v) = row.try_get::<i64, _>(i) {
                    serde_json::Value::Number(v.into())
                } else if let Ok(v) = row.try_get::<f64, _>(i) {
                    serde_json::json!(v)
                } else if let Ok(v) = row.try_get::<String, _>(i) {
                    serde_json::Value::String(v)
                } else {
                    serde_json::Value::Null
                }
            }).collect()
        })
        .collect();

    let row_count = json_rows.len();

    Ok(QueryResult {
        columns,
        rows: json_rows,
        row_count,
        truncated,
    })
}

#[tokio::main]
async fn main() -> Result<()> {
    let database_url = std::env::var("DATABASE_URL")
        .unwrap_or_else(|_| "sqlite:course.db".to_string());

    let pool: DbPool = Arc::new(
        SqlitePoolOptions::new()
            .max_connections(5)
            .connect(&database_url)
            .await?
    );

    let pool_for_tables = pool.clone();
    let pool_for_query = pool.clone();

    let server = Server::builder()
        .name("db-explorer")
        .version("1.0.0")
        .capabilities(ServerCapabilities {
            tools: Some(ToolCapabilities::default()),
            ..Default::default()
        })
        .tool("list_tables", TypedTool::new("list_tables", move |_: ListTablesInput| {
            let pool = pool_for_tables.clone();
            Box::pin(async move {
                let tables = list_tables(&pool).await?;
                Ok(serde_json::to_value(tables)?)
            })
        }))
        .tool("query", TypedTool::new("query", move |input: QueryInput| {
            let pool = pool_for_query.clone();
            Box::pin(async move {
                let result = execute_query(&pool, input).await?;
                Ok(serde_json::to_value(result)?)
            })
        }))
        .build()?;

    println!("Database explorer ready! Connected to: {}", database_url);
    Ok(())
}
"""

explanation = """
This solution demonstrates key patterns for database MCP servers:

**1. Connection Pooling**
Using `Arc<Pool<Sqlite>>` allows sharing the connection pool across tools:
- Connection reuse improves performance
- Pool limits prevent resource exhaustion
- Arc enables safe concurrent access

**2. Read-Only Validation**
```rust
if !trimmed.starts_with("SELECT") {
    return Err(...)
}
```
This is a first line of defense. Production servers need more:
- Block subqueries with dangerous operations
- Use database-level read-only connections
- Allowlist specific tables/columns

**3. Automatic LIMIT**
Adding LIMIT when not present prevents:
- Memory exhaustion from huge result sets
- Long-running queries blocking the pool
- Poor user experience from overwhelming output

**4. Truncation Detection**
Fetching `limit + 1` rows lets us tell the AI that more data exists:
```rust
let truncated = rows.len() > input.limit as usize;
```
This helps the AI ask follow-up questions or paginate.

**5. Flexible Type Handling**
SQLite is dynamically typed, so we try multiple types:
```rust
if let Ok(v) = row.try_get::<i64, _>(i) { ... }
else if let Ok(v) = row.try_get::<f64, _>(i) { ... }
```

**Why This Pattern for AI?**
- `columns` metadata helps AI understand the data structure
- `truncated` flag prompts AI to fetch more if needed
- `row_count` provides quick summary without parsing rows
- Structured JSON works better than raw text for analysis
"""

[exercise.ai_instructions]
role = """
You are helping a student build their first database MCP server. This bridges
their calculator/greeter experience to real-world enterprise scenarios.
"""

approach = """
1. CONNECT TO CONCEPTS (5 minutes):
   - Link to their calculator experience: "Remember handling errors? Database
     queries need even more validation."
   - Discuss why enterprises need database access for AI

2. DATABASE FUNDAMENTALS (5 minutes):
   - Explain connection pooling if they haven't used sqlx before
   - Show how sqlite_master stores metadata
   - Explain async database operations

3. IMPLEMENT IN STAGES (20 minutes):
   a) list_tables first - simpler, builds confidence
   b) execute_query - more complex, focus on validation
   c) Output formatting - ensure AI-friendly structure

4. TEST WITH REAL DATA:
   - If MCP inspector available, test against course.db
   - Try queries like "SELECT * FROM exercises LIMIT 5"
   - Demonstrate how truncation feedback works

5. SECURITY DISCUSSION:
   - Ask: "What if someone passed 'SELECT * FROM users; DROP TABLE users'?"
   - Discuss why SELECT validation alone isn't enough
   - Preview parameterized queries (next exercise)
"""

watch_for = [
    "Forgetting to clone Arc for closures",
    "Not handling the async nature of sqlx operations",
    "Missing the Row and Column imports for sqlx",
    "Trying to format SQL injection attacks - redirect to next exercise",
]

production_note = """
In production database servers, you would also:
- Use a read-only database connection/user
- Implement query timeout
- Add audit logging for all queries
- Consider caching for expensive queries
- Add table/column allowlists
- Parameterize any user input in query building
"""

[exercise.reflection]
questions = [
    "Why do we use Arc to share the database pool between tools?",
    "What are the limitations of checking 'starts_with SELECT'?",
    "How does the truncation feedback help an AI assistant?",
    "What would change if you used PostgreSQL instead of SQLite?",
]

expected_insights = [
    "Arc provides thread-safe reference counting for sharing across async tasks",
    "Subqueries, CTEs, and UNION could contain dangerous operations after SELECT",
    "AI can ask for more data or adjust queries knowing results were cut off",
    "Different system tables, different query syntax for metadata, connection strings",
]
