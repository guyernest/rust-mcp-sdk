# Exercise: Pagination Patterns for Large Results
#
# This exercise teaches students to handle large database results safely
# and efficiently - a critical skill for production MCP servers.

[exercise]
id = "ch03-03-pagination-patterns"
title = "Implementing Pagination for Large Results"
type = "implementation"
chapter = "ch03"
difficulty = "intermediate"
estimated_minutes = 30
prerequisites = ["ch03-01-db-query-basics"]

[exercise.context]
description = """
Your database query tool from the previous exercise works great for small
result sets, but what happens when a table has millions of rows? Without
proper pagination:

- Memory exhaustion: Loading 10M rows into memory crashes your server
- Timeouts: Long queries block the connection pool
- Poor UX: AI assistants can't process massive JSON responses effectively

This exercise teaches cursor-based pagination - the production pattern for
handling large datasets efficiently. You'll learn why it's superior to
offset-based pagination and how to implement it safely.
"""

[exercise.learning_objectives]
thinking = [
    "Why offset pagination fails at scale (OFFSET 1000000 is slow)",
    "How cursor-based pagination maintains consistent performance",
    "Tradeoffs between different pagination strategies",
]

doing = [
    "Implement cursor-based pagination with a 'next' token",
    "Handle edge cases (empty results, last page, invalid cursors)",
    "Design API responses that guide AI assistants to fetch more",
]

[exercise.discussion]
prompts = [
    "If you have 10 million rows and an AI asks for 'all customers', what should happen?",
    "Why is 'OFFSET 999000 LIMIT 1000' slower than 'WHERE id > 999000 LIMIT 1000'?",
    "How should an MCP response indicate that more data is available?",
    "What makes a good pagination cursor? (hint: not just a page number)",
]

concepts = [
    "Cursor-based vs offset-based pagination",
    "Keyset pagination",
    "Stable iteration under concurrent modifications",
    "API design for pagination",
]

[exercise.starter_code]
language = "rust"
filename = "src/main.rs"
template = """
//! Paginated Database Query Tool
//!
//! Demonstrates production-ready pagination patterns for large datasets.

use pmcp::{Server, ServerCapabilities, ToolCapabilities};
use pmcp::server::TypedTool;
use serde::{Deserialize, Serialize};
use schemars::JsonSchema;
use anyhow::Result;
use sqlx::{Pool, Sqlite, sqlite::SqlitePoolOptions, Row, Column};
use std::sync::Arc;
use base64::{Engine as _, engine::general_purpose::STANDARD as BASE64};

type DbPool = Arc<Pool<Sqlite>>;

/// Input for paginated query
#[derive(Deserialize, JsonSchema)]
struct PaginatedQueryInput {
    /// Table to query (from allowlist)
    table: String,

    /// Columns to select (optional, defaults to all)
    columns: Option<Vec<String>>,

    /// Number of rows per page (max 100)
    #[serde(default = "default_page_size")]
    page_size: i32,

    /// Cursor from previous response (omit for first page)
    cursor: Option<String>,
}

fn default_page_size() -> i32 { 50 }

/// Output with pagination information
#[derive(Serialize)]
struct PaginatedResult {
    /// Column names
    columns: Vec<String>,

    /// Row data
    rows: Vec<Vec<serde_json::Value>>,

    /// Number of rows in this page
    count: usize,

    /// Cursor for next page (null if no more data)
    next_cursor: Option<String>,

    /// Human-readable pagination status for AI
    status: String,
}

/// Internal cursor structure
#[derive(Serialize, Deserialize)]
struct Cursor {
    /// Last ID seen
    last_id: i64,
    /// Table being queried (for validation)
    table: String,
}

impl Cursor {
    fn encode(&self) -> String {
        let json = serde_json::to_string(self).unwrap();
        BASE64.encode(json.as_bytes())
    }

    fn decode(encoded: &str) -> Result<Self> {
        let bytes = BASE64.decode(encoded)?;
        let json = String::from_utf8(bytes)?;
        Ok(serde_json::from_str(&json)?)
    }
}

// Allowlisted tables for security
const ALLOWED_TABLES: &[&str] = &["users", "orders", "products", "customers"];

/// Execute a paginated query
async fn paginated_query(pool: &DbPool, input: PaginatedQueryInput) -> Result<PaginatedResult> {
    // TODO: Implement paginated query
    //
    // Steps:
    // 1. Validate table is in allowlist
    // 2. If cursor provided, decode and validate it
    // 3. Build query with WHERE id > last_id ORDER BY id LIMIT page_size+1
    // 4. Execute query and extract results
    // 5. Check if there are more results (fetched page_size + 1)
    // 6. If more results, create next_cursor with last row's ID
    // 7. Return PaginatedResult with appropriate status message

    // Hints:
    // - Use the Cursor struct to encode/decode cursor state
    // - Fetch page_size + 1 rows to detect if more pages exist
    // - The status field should help AI understand pagination state

    todo!("Implement paginated_query")
}

/// Get total count for a table (for context)
async fn get_table_count(pool: &DbPool, table: &str) -> Result<i64> {
    if !ALLOWED_TABLES.contains(&table) {
        return Err(anyhow::anyhow!("Table not allowed: {}", table));
    }

    let count: (i64,) = sqlx::query_as(&format!("SELECT COUNT(*) FROM {}", table))
        .fetch_one(pool.as_ref())
        .await?;

    Ok(count.0)
}

#[tokio::main]
async fn main() -> Result<()> {
    let pool: DbPool = Arc::new(
        SqlitePoolOptions::new()
            .max_connections(5)
            .connect("sqlite:sample.db")
            .await?
    );

    let pool_query = pool.clone();

    let server = Server::builder()
        .name("paginated-db")
        .version("1.0.0")
        .capabilities(ServerCapabilities {
            tools: Some(ToolCapabilities::default()),
            ..Default::default()
        })
        .tool("query", TypedTool::new("query", move |input: PaginatedQueryInput| {
            let pool = pool_query.clone();
            Box::pin(async move {
                let result = paginated_query(&pool, input).await?;
                Ok(serde_json::to_value(result)?)
            })
        }))
        .build()?;

    println!("Paginated database server ready!");
    Ok(())
}
"""

[exercise.tests]
language = "rust"
run_command = "cargo test"
test_code = """
#[cfg(test)]
mod tests {
    use super::*;

    async fn create_test_pool() -> DbPool {
        let pool = SqlitePoolOptions::new()
            .max_connections(1)
            .connect("sqlite::memory:")
            .await
            .unwrap();

        // Create test table
        sqlx::query("CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT)")
            .execute(&pool)
            .await
            .unwrap();

        // Insert 10 test rows
        for i in 1..=10 {
            sqlx::query(&format!("INSERT INTO users (id, name) VALUES ({}, 'User {}')", i, i))
                .execute(&pool)
                .await
                .unwrap();
        }

        Arc::new(pool)
    }

    #[tokio::test]
    async fn test_first_page() {
        let pool = create_test_pool().await;
        let input = PaginatedQueryInput {
            table: "users".to_string(),
            columns: None,
            page_size: 3,
            cursor: None,
        };

        let result = paginated_query(&pool, input).await.unwrap();

        assert_eq!(result.count, 3);
        assert!(result.next_cursor.is_some());
        assert!(result.status.contains("more"));
    }

    #[tokio::test]
    async fn test_continue_with_cursor() {
        let pool = create_test_pool().await;

        // First page
        let first = paginated_query(&pool, PaginatedQueryInput {
            table: "users".to_string(),
            columns: None,
            page_size: 3,
            cursor: None,
        }).await.unwrap();

        // Second page using cursor
        let second = paginated_query(&pool, PaginatedQueryInput {
            table: "users".to_string(),
            columns: None,
            page_size: 3,
            cursor: first.next_cursor,
        }).await.unwrap();

        assert_eq!(second.count, 3);
        assert!(second.next_cursor.is_some());

        // Verify no overlap - first row of second page should be after last of first
        let first_ids: Vec<i64> = first.rows.iter()
            .map(|r| r[0].as_i64().unwrap())
            .collect();
        let second_ids: Vec<i64> = second.rows.iter()
            .map(|r| r[0].as_i64().unwrap())
            .collect();

        assert!(first_ids.iter().max().unwrap() < second_ids.iter().min().unwrap());
    }

    #[tokio::test]
    async fn test_last_page() {
        let pool = create_test_pool().await;

        // Skip to near the end
        let mut cursor = None;
        loop {
            let result = paginated_query(&pool, PaginatedQueryInput {
                table: "users".to_string(),
                columns: None,
                page_size: 4,
                cursor,
            }).await.unwrap();

            if result.next_cursor.is_none() {
                // Last page should indicate completion
                assert!(result.status.to_lowercase().contains("complete")
                    || result.status.to_lowercase().contains("all")
                    || result.status.to_lowercase().contains("end"));
                break;
            }
            cursor = result.next_cursor;
        }
    }

    #[tokio::test]
    async fn test_invalid_table() {
        let pool = create_test_pool().await;
        let input = PaginatedQueryInput {
            table: "secrets".to_string(),  // Not in allowlist
            columns: None,
            page_size: 10,
            cursor: None,
        };

        let result = paginated_query(&pool, input).await;
        assert!(result.is_err());
    }

    #[tokio::test]
    async fn test_invalid_cursor() {
        let pool = create_test_pool().await;
        let input = PaginatedQueryInput {
            table: "users".to_string(),
            columns: None,
            page_size: 10,
            cursor: Some("invalid_base64!!".to_string()),
        };

        let result = paginated_query(&pool, input).await;
        // Should either error or ignore invalid cursor
        assert!(result.is_err() || result.unwrap().count > 0);
    }

    #[tokio::test]
    async fn test_cursor_table_mismatch() {
        let pool = create_test_pool().await;

        // Create cursor for 'users'
        let cursor = Cursor { last_id: 5, table: "users".to_string() };

        // Try to use it for 'orders' (different table)
        let input = PaginatedQueryInput {
            table: "orders".to_string(),
            columns: None,
            page_size: 10,
            cursor: Some(cursor.encode()),
        };

        // Should fail because cursor table doesn't match query table
        let result = paginated_query(&pool, input).await;
        assert!(result.is_err());
    }
}
"""

[exercise.hints]
level_1 = """
Start by validating the table:
```rust
if !ALLOWED_TABLES.contains(&input.table.as_str()) {
    return Err(anyhow::anyhow!("Table not allowed"));
}
```
"""

level_2 = """
Build the query with cursor support:
```rust
let start_id = if let Some(cursor_str) = &input.cursor {
    let cursor = Cursor::decode(cursor_str)?;
    if cursor.table != input.table {
        return Err(anyhow::anyhow!("Cursor table mismatch"));
    }
    cursor.last_id
} else {
    0
};

let query = format!(
    "SELECT * FROM {} WHERE id > {} ORDER BY id LIMIT {}",
    input.table, start_id, input.page_size + 1
);
```
"""

level_3 = """
Complete implementation:
```rust
async fn paginated_query(pool: &DbPool, input: PaginatedQueryInput) -> Result<PaginatedResult> {
    // Validate table
    if !ALLOWED_TABLES.contains(&input.table.as_str()) {
        return Err(anyhow::anyhow!("Table '{}' not allowed", input.table));
    }

    // Limit page size
    let page_size = input.page_size.min(100);

    // Decode cursor
    let start_id = match &input.cursor {
        Some(c) => {
            let cursor = Cursor::decode(c)?;
            if cursor.table != input.table {
                return Err(anyhow::anyhow!("Cursor was for different table"));
            }
            cursor.last_id
        }
        None => 0,
    };

    // Build and execute query
    let query = format!(
        "SELECT * FROM {} WHERE id > {} ORDER BY id LIMIT {}",
        input.table, start_id, page_size + 1
    );

    let rows = sqlx::query(&query)
        .fetch_all(pool.as_ref())
        .await?;

    // Check for more results
    let has_more = rows.len() > page_size as usize;
    let rows: Vec<_> = rows.into_iter().take(page_size as usize).collect();

    // Build result...
}
```
"""

hint_policy = "after_stuck"
stuck_threshold_minutes = 7

[exercise.solution]
unlock_policy = "after_attempt"
solution_code = """
use pmcp::{Server, ServerCapabilities, ToolCapabilities};
use pmcp::server::TypedTool;
use serde::{Deserialize, Serialize};
use schemars::JsonSchema;
use anyhow::Result;
use sqlx::{Pool, Sqlite, sqlite::SqlitePoolOptions, Row, Column};
use std::sync::Arc;
use base64::{Engine as _, engine::general_purpose::STANDARD as BASE64};

type DbPool = Arc<Pool<Sqlite>>;

#[derive(Deserialize, JsonSchema)]
struct PaginatedQueryInput {
    table: String,
    columns: Option<Vec<String>>,
    #[serde(default = "default_page_size")]
    page_size: i32,
    cursor: Option<String>,
}

fn default_page_size() -> i32 { 50 }

#[derive(Serialize)]
struct PaginatedResult {
    columns: Vec<String>,
    rows: Vec<Vec<serde_json::Value>>,
    count: usize,
    next_cursor: Option<String>,
    status: String,
}

#[derive(Serialize, Deserialize)]
struct Cursor {
    last_id: i64,
    table: String,
}

impl Cursor {
    fn encode(&self) -> String {
        let json = serde_json::to_string(self).unwrap();
        BASE64.encode(json.as_bytes())
    }

    fn decode(encoded: &str) -> Result<Self> {
        let bytes = BASE64.decode(encoded)?;
        let json = String::from_utf8(bytes)?;
        Ok(serde_json::from_str(&json)?)
    }
}

const ALLOWED_TABLES: &[&str] = &["users", "orders", "products", "customers"];

async fn paginated_query(pool: &DbPool, input: PaginatedQueryInput) -> Result<PaginatedResult> {
    // Validate table
    if !ALLOWED_TABLES.contains(&input.table.as_str()) {
        return Err(anyhow::anyhow!("Table '{}' not in allowlist", input.table));
    }

    // Limit page size for safety
    let page_size = input.page_size.min(100).max(1);

    // Decode cursor if provided
    let start_id = match &input.cursor {
        Some(cursor_str) => {
            let cursor = Cursor::decode(cursor_str)
                .map_err(|_| anyhow::anyhow!("Invalid cursor format"))?;
            if cursor.table != input.table {
                return Err(anyhow::anyhow!(
                    "Cursor was created for table '{}', not '{}'",
                    cursor.table, input.table
                ));
            }
            cursor.last_id
        }
        None => 0,
    };

    // Build column list
    let columns_sql = match &input.columns {
        Some(cols) => {
            // Validate column names (simple alphanumeric check)
            for col in cols {
                if !col.chars().all(|c| c.is_alphanumeric() || c == '_') {
                    return Err(anyhow::anyhow!("Invalid column name: {}", col));
                }
            }
            cols.join(", ")
        }
        None => "*".to_string(),
    };

    // Build query with cursor-based pagination
    // Fetch one extra to detect if more pages exist
    let query = format!(
        "SELECT {} FROM {} WHERE id > ? ORDER BY id ASC LIMIT ?",
        columns_sql, input.table
    );

    let rows = sqlx::query(&query)
        .bind(start_id)
        .bind(page_size + 1)
        .fetch_all(pool.as_ref())
        .await?;

    // Check if there are more results
    let has_more = rows.len() > page_size as usize;
    let rows: Vec<_> = rows.into_iter().take(page_size as usize).collect();

    // Extract column names
    let columns: Vec<String> = if !rows.is_empty() {
        rows[0].columns().iter().map(|c| c.name().to_string()).collect()
    } else {
        vec![]
    };

    // Convert rows to JSON values
    let json_rows: Vec<Vec<serde_json::Value>> = rows
        .iter()
        .map(|row| {
            (0..row.columns().len()).map(|i| {
                if let Ok(v) = row.try_get::<i64, _>(i) {
                    serde_json::Value::Number(v.into())
                } else if let Ok(v) = row.try_get::<f64, _>(i) {
                    serde_json::json!(v)
                } else if let Ok(v) = row.try_get::<String, _>(i) {
                    serde_json::Value::String(v)
                } else {
                    serde_json::Value::Null
                }
            }).collect()
        })
        .collect();

    let count = json_rows.len();

    // Create next cursor if there are more results
    let next_cursor = if has_more && !json_rows.is_empty() {
        // Get the ID from the last row (assumes first column is id)
        let last_row = json_rows.last().unwrap();
        let last_id = last_row[0].as_i64()
            .ok_or_else(|| anyhow::anyhow!("First column must be numeric id"))?;

        Some(Cursor {
            last_id,
            table: input.table.clone(),
        }.encode())
    } else {
        None
    };

    // Create human-readable status for AI
    let status = if count == 0 {
        "No results found.".to_string()
    } else if next_cursor.is_some() {
        format!(
            "Returned {} rows. More data available - use the next_cursor to continue.",
            count
        )
    } else {
        format!("Returned {} rows. This is all available data.", count)
    };

    Ok(PaginatedResult {
        columns,
        rows: json_rows,
        count,
        next_cursor,
        status,
    })
}

#[tokio::main]
async fn main() -> Result<()> {
    let pool: DbPool = Arc::new(
        SqlitePoolOptions::new()
            .max_connections(5)
            .connect("sqlite:sample.db")
            .await?
    );

    let pool_query = pool.clone();

    let server = Server::builder()
        .name("paginated-db")
        .version("1.0.0")
        .capabilities(ServerCapabilities {
            tools: Some(ToolCapabilities::default()),
            ..Default::default()
        })
        .tool("query", TypedTool::new("query", move |input: PaginatedQueryInput| {
            let pool = pool_query.clone();
            Box::pin(async move {
                let result = paginated_query(&pool, input).await?;
                Ok(serde_json::to_value(result)?)
            })
        }))
        .build()?;

    println!("Paginated database server ready!");
    Ok(())
}
"""

explanation = """
This solution demonstrates production-ready cursor-based pagination:

**Why Cursor-Based Over Offset-Based?**

Offset-based: `SELECT * FROM users LIMIT 10 OFFSET 1000000`
- Database must scan and skip 1,000,000 rows
- Gets slower as offset increases
- Results can shift if data changes between pages

Cursor-based: `SELECT * FROM users WHERE id > 1000000 LIMIT 10`
- Uses index to jump directly to the starting point
- Consistent O(1) performance regardless of position
- Stable iteration even with concurrent modifications

**Key Implementation Details:**

1. **Opaque Cursors**
   We encode cursor state as base64 JSON. This:
   - Hides implementation details from clients
   - Allows adding fields without breaking clients
   - Validates that cursor matches the current query

2. **Fetch N+1 Pattern**
   ```rust
   let rows = ... LIMIT page_size + 1
   let has_more = rows.len() > page_size;
   let rows = rows.into_iter().take(page_size);
   ```
   This efficiently detects whether more pages exist without
   an extra COUNT query.

3. **Table Validation in Cursor**
   The cursor includes the table name. This prevents attacks like:
   - Get cursor from `users` table
   - Use it to paginate `admin_secrets` table
   - Potentially access restricted data

4. **Human-Readable Status**
   The `status` field helps AI assistants understand pagination:
   - "More data available - use next_cursor"
   - "This is all available data"
   This guides the AI to continue fetching or stop.

**Security Considerations:**
- Table allowlist prevents access to arbitrary tables
- Column name validation prevents SQL injection
- Page size limits prevent memory exhaustion
- Parameterized queries for all values
"""

[exercise.ai_instructions]
role = """
You are helping a student understand pagination patterns. This builds on their
database query knowledge and introduces a critical production concept.
"""

approach = """
1. MOTIVATE THE PROBLEM (5 minutes):
   - Ask: "What happens if you SELECT * from a table with 10 million rows?"
   - Discuss memory, time, and UX implications
   - Why can't AI assistants process 10MB JSON responses effectively?

2. OFFSET VS CURSOR DISCUSSION (5 minutes):
   - Show why OFFSET 1000000 is slow (must skip rows)
   - Explain how WHERE id > 1000000 uses the index
   - Discuss the "shifting results" problem with offset

3. IMPLEMENT IN STAGES (15 minutes):
   a) Basic cursor encoding/decoding
   b) Query building with cursor
   c) Next cursor generation
   d) Human-readable status messages

4. TESTING AND EDGE CASES:
   - Empty results
   - Last page (no more cursor)
   - Invalid cursor
   - Wrong table in cursor

5. ADVANCED DISCUSSION:
   - What if there's no 'id' column?
   - Compound cursors (multiple sort columns)
   - Consistency guarantees
"""

watch_for = [
    "Forgetting the +1 for detecting more pages",
    "Not handling empty result sets",
    "Missing table validation in cursor",
    "Using offset instead of cursor approach",
]

production_note = """
Production pagination often needs:
- Support for non-integer primary keys (UUIDs)
- Compound cursors for multi-column sorting
- Cursor expiration for long-lived operations
- Total count estimates (not exact, for UI)
- Support for backward pagination (previous page)
"""

[exercise.reflection]
questions = [
    "Why do we include the table name in the cursor?",
    "What would happen if rows were deleted between page fetches?",
    "How would you support sorting by a non-unique column?",
    "Why is the cursor base64-encoded JSON instead of just an ID?",
]

expected_insights = [
    "Prevents using a cursor from table A to query table B - security boundary",
    "Cursor-based pagination handles this gracefully - you might skip deleted rows but never see duplicates",
    "Need compound cursor with (sort_column, id) to handle ties in sort order",
    "Allows adding fields later (sort direction, etc.) without breaking clients; hides implementation",
]
